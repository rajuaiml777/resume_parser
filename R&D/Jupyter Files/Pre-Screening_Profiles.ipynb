{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF text Extracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Anil Sahoo \n",
      "Machine Learning Engineer \n",
      "Email: anilsahoo.aiml@gmail.com \n",
      "\n",
      "LinkedIn: https://www.linkedin.com/in/anil-sahoo-ba313422a/ \n",
      "\n",
      "Mobile: +91-9322594228 \n",
      "\n",
      " \n",
      "\n",
      "• \n",
      "\n",
      "PROFESSIONAL BACKGROUND: \n",
      "\n",
      "•  Total 2+ years of experience in IT Industry. \n",
      "•  Currently working as Full time Machine Learning Engineer at Sysark Datasol Pvt. Ltd.  \n",
      "\n",
      "since Nov 2019. \n",
      " Machine Learning Engineer with  2 years of experience in  Machine Learning,Natural \n",
      "Language  Processing  (NLP),  Data  mining,  Artificial  Intelligence,  Statistic \n",
      "Modeling, Predictive Modeling, Data & Text Analytics. \n",
      "\n",
      "•  Working  Knowledge  in  applied  Mathematics  such  as  Linear  Algebra,  Calculus  and \n",
      "\n",
      "Statistics. \n",
      "\n",
      "•  Expertise  in  transforming  business  requirements  into  analytical  models,  designing \n",
      "algorithms, building models, developing data mining and reporting solutions that scales \n",
      "across massive volume of structured and unstructured data \n",
      "\n",
      "•  Adept in programming languages like Python and SQL. \n",
      " \n",
      " EDUCATIONAL QUALIFICATION: \n",
      "\n",
      "Name of the Institution \n",
      "\n",
      "     Board/                    \n",
      "University \n",
      "\n",
      " Year of                                        \n",
      "  Passing \n",
      "\n",
      "Percentage \n",
      " \n",
      "\n",
      "Indira gandhi institute of \n",
      "technology,Dhenkanal,Odisha \n",
      "\n",
      "2019 \n",
      "\n",
      "78% \n",
      "\n",
      "BIJU PATTNAIK \n",
      "UNIVERSITY OF \n",
      "TECHNOLOGY, \n",
      "ROURKELA \n",
      "\n",
      "Bhadrak junior college, Bhadrak  CHSE(Odisha) \n",
      "\n",
      "2015 \n",
      "\n",
      "82.5% \n",
      "\n",
      " Utakal mani ucha bidya \n",
      "pitha,Bandhatia \n",
      "\n",
      "BSE(Odisha) \n",
      "\n",
      "2013 \n",
      "\n",
      "88.7% \n",
      "\n",
      " \n",
      "\n",
      "Name of the \n",
      "Course \n",
      "\n",
      "B.Tech \n",
      "\n",
      " \n",
      "\n",
      "XIIth \n",
      "\n",
      "                 Xth \n",
      "\n",
      " \n",
      " \n",
      "  KEY SKILLS: \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "• \n",
      "\n",
      " Python                                                           \n",
      " Machine learning                                           \n",
      " Statistics \n",
      " EDA \n",
      " NLP \n",
      " MS SQL Server \n",
      "\n",
      "\f",
      " \n",
      "\n",
      "       \n",
      "  TECHNICAL SKILLS:  . \n",
      " \n",
      "•  Python  -  Developed  various  regression  and  classification  models  of  ML  using  pandas, \n",
      "\n",
      "numpy, scikit learn etc, and visualization using Seaborn and MatlabPlot libraries.   \n",
      "\n",
      "•  Statistics  and  Mathmetics    -  Strong  knowledge  on  Descriptive  statistics  ,  Inferential \n",
      "statistics,  Probability,  ANOVA,  Chi-Square  test,  t-test,  z-test,  p-value,  CI,  Hypothesis-\n",
      "test,Sampling, Data distributions, Single and multivariate calculus, Linear algebra  \n",
      "\n",
      "•  Machine  Learning-(supervised  learning) Strong knowledge of various ML regression \n",
      "and  classification  algorithms  such  as  Linear  and  logistic  regression,  Gradient  descent, \n",
      "Decision  tree,  Random  forest,  SVM  ,  Naive  Bayes,  KNN  ,XGBoost,  Lasso  and  Ridge \n",
      "regression  ,Hyperparameter  tunig  and  model  performance  technique  such  as  Confusion \n",
      "matrix,  AUC  and  ROC  ,  IG  ,Gini,  Confidence  Interval,  ANOVA,RMSE  and  also  have \n",
      "knowledge  on  validating  the  model  by  Cross  Validation.(unsupervised  learning)-K-\n",
      "means clustering,  DBSCAN clustering, Heirarchcal clustering \n",
      "\n",
      "•  NLP(Nltk/spaCy)  -  Tokenization, Stemming, Lematization ,Stop Words, Part of Speech , \n",
      "\n",
      "Named-entity recognition , Bag-of-Words, TF-IDF, Word2vec, Text Summerization \n",
      "\n",
      " \n",
      "PROJECTS:   \n",
      " \n",
      "PROJECT 1: Customer Feedback Analysis \n",
      " \n",
      "DURATION:May 2021 - Till Date \n",
      " \n",
      "OVERVIEW: Feedback  analysis involves identifying  the needs  and frustrations  of customers, \n",
      "so  that  businesses  can  improve  customer  satisfaction  and  reduce  churn  .It's  often  done \n",
      "automatically,  enabling  companies  to  sort  huge  amounts  of  data  from  various  channels  in  a \n",
      "timely  and  accurate  way.  An  NLP  engine  scrapes  customers'  conversations  and  feedback  on \n",
      "websites,  social  media  channels  and  mobile  apps  etc.  and  then  analyze  these  conversations. \n",
      "conduct \n",
      "required, \n",
      "Depending \n",
      "binary(positive/negative)analysis  and  sentiment  analysis  on  various  emotions \n",
      "like \n",
      "Happy,Angry,Sadness and Trust etc.  \n",
      " \n",
      "RESPONSIBILITIES:  \n",
      "•  Cleansing  and  transforming  the  text  dataset  to  create  a  comprehensive  dataset  which \n",
      "\n",
      "intelligence \n",
      "\n",
      "the  NLP \n",
      "\n",
      "engine \n",
      "\n",
      "can \n",
      "\n",
      "the \n",
      "\n",
      "on \n",
      "\n",
      "contain all the possible hypothesis. \n",
      "\n",
      "\f",
      "•  Text  preprocessing  is  one  of  the  most  important  step  in  the  field  of  Natural  language \n",
      "processing .So i clean the text data by removing  puncuations , stopwords and then covert \n",
      "the text data into lower case , apply lemmatization  \n",
      " After  clean  the  text  data  then  i  performed  data  transformation  because  machine  can't \n",
      "understands  the  text  data  so  i  use  Bag  of  Words  ,which  converts  all  the  text  data  into \n",
      "numerical dataset  \n",
      "\n",
      "• \n",
      "\n",
      " \n",
      "TECHNOLOGIES USED: Python ,Spyder, Sklearn, NLP, NLTK, spaCy. \n",
      " \n",
      "PROJECT 2: Loan Defaulter Risk Analysis \n",
      " \n",
      "DURATION:  Feb 2020 -Mar 2021 \n",
      " \n",
      "ROLE:  ML Engineer \n",
      " \n",
      "OVERVIEW: The goal of this project is to build a machine learning model that can predict if \n",
      "a  person  will  default  on  the  loan  based  on  the  loan  and  personal  information.  Also  this \n",
      "project aims to develop an understanding of risk analytics in banking and financial services \n",
      "and  understand  how  data  is  used  to  minimize  the  risk  of  losing  money  while  lending  to \n",
      "customers. \n",
      " \n",
      "RESPONSIBILITIES:   \n",
      "•  Worked  on  client  during  projects  and  understanding  their  requirement  was  the  best \n",
      "\n",
      "•  As  my  dataset  is  supervised  learning  and  my  problem  statement  is  classification \n",
      "\n",
      "way for me to build  an optimal solutions. \n",
      "\n",
      "problem that's why i used classification. \n",
      "\n",
      "•  Data  preprocessing  steps  are  done  -  Acquire  the  dataset,  import  the  required \n",
      "libraries,import  the  dataset,indentify  and  handling  missing  values,encoding  the \n",
      "categorical variable,spliting the dataset, feature scaling. \n",
      "\n",
      "•  Apply  machine  learning  classification  model  like  -Logistic  regression,  SVC,  Decision \n",
      "\n",
      "tree, Random forest, Xgboost. \n",
      "\n",
      " \n",
      "TECHNOLOGIES USED:  Python, EDA , Data Visualization,  Data Cleaning, Classification \n",
      "algorithms \n",
      " \n",
      " \n",
      "STRENGTH: \n",
      "•  Hard working \n",
      "\n",
      "•  Self motivated \n",
      "\n",
      "•  Positive attitude \n",
      "\n",
      "•  Quick learner \n",
      "\n",
      "\f",
      "HOBBIES: \n",
      "•  Listening songs \n",
      "\n",
      "•  Playing games \n",
      "\n",
      "PERSONAL PROFILE: \n",
      " \n",
      "       Father’s Name \n",
      "\n",
      "     \n",
      "\n",
      ":  Chakradhar sahoo \n",
      "\n",
      "       Date of Birth              \n",
      "\n",
      ":  1st may 1998 \n",
      "\n",
      "       Gender                       \n",
      "\n",
      "            :   Male   \n",
      "\n",
      "       Language Known       \n",
      "\n",
      ":  English,Hindi,Odia \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "       Permanent Address                     \n",
      "\n",
      ":  At:-Radhaballav pur, Post:-Bandhatia, \n",
      "\n",
      "                                                                    Via:-Dhamnagar, Bhadrak, Odisha, \n",
      "\n",
      "                                                                    Pin code:-  756117,INDIA         \n",
      "\n",
      "       Present Address                                :  B-201, Ostia Destination, Moshi, Pune,MH \n",
      "\n",
      "                                                                   Pin code:-  412105, INDIA         \n",
      "\n",
      " \n",
      "\n",
      "  DECLARATION: \n",
      " \n",
      "I hereby declare that the above particulars furnished by me are true to the best of my knowledge \n",
      "and belief.  \n",
      "                             \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "                                  Anil Sahoo         \n",
      "\n",
      "     \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    " \n",
    "text = extract_text(r\"C:\\Users\\manikantar\\Downloads\\AnilSahoo[2_2].pdf\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign Fraud Detection\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What AI can do ?\n",
      "\n",
      "\n",
      "\n",
      "BASED ON TEXT DATA:\n",
      "\n",
      "\n",
      "\n",
      "Sentiment Analysis of campaign descriptions\n",
      "\n",
      "Complexity of language used based on words\n",
      "\n",
      "Form of the text (How user described the scenario)\n",
      "\n",
      "Words importance (by this we can add some conditions to decide which type of text it is)\n",
      "\n",
      "Named entity recognition to extract specific type of words from the description.\n",
      "\n",
      "\n",
      "\n",
      "BASED ON IMAGE DATA (Only if Enhancement needed later) :\n",
      "\n",
      "\n",
      "\n",
      "We are able to capture the emotions in the images by detecting the faces in the images.\n",
      "\n",
      "\n",
      "\n",
      "REQUIREMENTS/ DEPENDENCIES :\n",
      "\n",
      "\n",
      "\n",
      "Need to collect data from fund raising web pages.\n",
      "\n",
      "Manually label the extracted text data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NOTE 1 : You can expect results from AI end only if we have campaigns data which is properly labeled.\n",
      "\n",
      "\n",
      "\n",
      "NOTE 2:  If we get name,email,longitude, latitude and other details if any then we will try to do some advancements usage of AI in fraud campaign detection.\n"
     ]
    }
   ],
   "source": [
    "my_text = docx2txt.process(r\"C:\\Users\\manikantar\\Downloads\\Campaign Fraud Detection.docx\")\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PDF to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_1112/454973442.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\MANIKA~1\\AppData\\Local\\Temp/ipykernel_1112/454973442.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    pages.save(r'G:\\PreScreening\\resumes\\New folder\\','JPEG')\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "pages = convert_from_path(r'G:\\PreScreening\\resumes\\AdapaSuryaAnishKumar[4_0].pdf',500)\n",
    "i=1\n",
    "for page in range(len(pages)):\n",
    "    page.save((r'G:\\PreScreening\\resumes\\New folder\\filename_{}.png'.format(i) +'.jpg', 'JPEG'))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOC to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aspose.words as aw\n",
    "import watermark\n",
    "\n",
    "doc = aw.Document(r'C:\\Users\\manikantar\\Downloads\\BuchhibabuRachakonda.docx')\n",
    "i=1          \n",
    "for page in range(0, doc.page_count):\n",
    "    extractedPage = doc.extract_pages(page, 1)\n",
    "    extractedPage.save(r'C:\\Users\\manikantar\\Downloads\\sample_{}.png'.format(i))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paid Conversion Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertapi.api_secret = 'https://v2.convertapi.com/convert/pdf/to/jpg?StoreFile=true'\n",
    "convertapi.convert('jpg', {\n",
    "    'File': r'C:\\Users\\manikantar\\Downloads\\RajuG[3_5].pdf'\n",
    "}, from_format = 'pdf').save_files('r'C:\\Users\\manikantar\\Downloads\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Node.JS  Python  C#  Java  PHP  Objective-C  Ruby  Apex  C/C++  cURL  Swift  JavaScript  Go\n",
    "+ Install Python SDK\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import cloudmersive_convert_api_client\n",
    "from cloudmersive_convert_api_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "\n",
    "# Configure API key authorization: Apikey\n",
    "configuration = cloudmersive_convert_api_client.Configuration()\n",
    "configuration.api_key['Apikey'] = 'YOUR_API_KEY'\n",
    "\n",
    "\n",
    "\n",
    "# create an instance of the API class\n",
    "api_instance = cloudmersive_convert_api_client.ConvertImageApi(cloudmersive_convert_api_client.ApiClient(configuration))\n",
    "format1 = 'format1_example' # str | Input file format as a 3+ letter file extension.  You can also provide UNKNOWN for unknown file formats. Supported formats include AAI, ART, ARW, AVS, BPG, BMP, BMP2, BMP3, BRF, CALS, CGM, CIN, CMYK, CMYKA, CR2, CRW, CUR, CUT, DCM, DCR, DCX, DDS, DIB, DJVU, DNG, DOT, DPX, EMF, EPDF, EPI, EPS, EPS2, EPS3, EPSF, EPSI, EPT, EXR, FAX, FIG, FITS, FPX, GIF, GPLT, GRAY, HDR, HEIC, HPGL, HRZ, ICO, ISOBRL, ISBRL6, JBIG, JNG, JP2, JPT, J2C, J2K, JPEG/JPG, JXR, MAT, MONO, MNG, M2V, MRW, MTV, NEF, ORF, OTB, P7, PALM, PAM, PBM, PCD, PCDS, PCL, PCX, PDF, PEF, PES, PFA, PFB, PFM, PGM, PICON, PICT, PIX, PNG, PNG8, PNG00, PNG24, PNG32, PNG48, PNG64, PNM, PPM, PSB, PSD, PTIF, PWB, RAD, RAF, RGB, RGBA, RGF, RLA, RLE, SCT, SFW, SGI, SID, SUN, SVG, TGA, TIFF, TIM, UIL, VIFF, VICAR, VBMP, WDP, WEBP, WPG, X, XBM, XCF, XPM, XWD, X3F, YCbCr, YCbCrA, YUV\n",
    "format2 = 'format2_example' # str | Output (convert to this format) file format as a 3+ letter file extension.  Supported formats include AAI, ART, ARW, AVS, BPG, BMP, BMP2, BMP3, BRF, CALS, CGM, CIN, CMYK, CMYKA, CR2, CRW, CUR, CUT, DCM, DCR, DCX, DDS, DIB, DJVU, DNG, DOT, DPX, EMF, EPDF, EPI, EPS, EPS2, EPS3, EPSF, EPSI, EPT, EXR, FAX, FIG, FITS, FPX, GIF, GPLT, GRAY, HDR, HEIC, HPGL, HRZ, ICO, ISOBRL, ISBRL6, JBIG, JNG, JP2, JPT, J2C, J2K, JPEG/JPG, JXR, MAT, MONO, MNG, M2V, MRW, MTV, NEF, ORF, OTB, P7, PALM, PAM, PBM, PCD, PCDS, PCL, PCX, PDF, PEF, PES, PFA, PFB, PFM, PGM, PICON, PICT, PIX, PNG, PNG8, PNG00, PNG24, PNG32, PNG48, PNG64, PNM, PPM, PSB, PSD, PTIF, PWB, RAD, RAF, RGB, RGBA, RGF, RLA, RLE, SCT, SFW, SGI, SID, SUN, SVG, TGA, TIFF, TIM, UIL, VIFF, VICAR, VBMP, WDP, WEBP, WPG, X, XBM, XCF, XPM, XWD, X3F, YCbCr, YCbCrA, YUV\n",
    "input_file = '/path/to/inputfile' # file | Input file to perform the operation on.\n",
    "\n",
    "try:\n",
    "    # Image format conversion\n",
    "    api_response = api_instance.convert_image_image_format_convert(format1, format2, input_file)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling ConvertImageApi->convert_image_image_format_convert: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text extrater from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      " \n",
      "Anil Sahoo\n",
      " \n",
      "Machine Learning Engineer\n",
      " \n",
      "Email: anilsahoo.aiml@gmail.com\n",
      " \n",
      "LinkedIn: \n",
      "https://www.linkedin.com/in/anil\n",
      "-\n",
      "sahoo\n",
      "-\n",
      "ba313422a/\n",
      " \n",
      "Mobile: \n",
      "+91\n",
      "-\n",
      "9322594228\n",
      " \n",
      " \n",
      "PROFESSIONAL BACKGROUND:\n",
      " \n",
      "\n",
      " \n",
      "Total \n",
      "2+ years \n",
      "of experience in IT Industry.\n",
      " \n",
      "\n",
      " \n",
      "Currently working as Full time Machine Learning Engineer at \n",
      "Sysark Datasol Pvt. Ltd.  \n",
      "since Nov 2019.\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Machine Learning Engineer with \n",
      "2 \n",
      "years of experience in \n",
      "Machine Learning,Natural \n",
      "La\n",
      "nguage Processing (NLP), Data mining, Artificial Intelligence, Statistic \n",
      "Modeling, Predictive Modeling, Data & Text Analytics\n",
      ".\n",
      " \n",
      "\n",
      " \n",
      "Working Knowledge in applied Mathematics such as Linear Algebra, Calculus and \n",
      "Statistics.\n",
      " \n",
      "\n",
      " \n",
      "Expertise in transforming business requ\n",
      "irements into analytical models, designing \n",
      "algorithms, building models, developing data mining and reporting solutions that scales \n",
      "across massive volume of structured and unstructured data\n",
      " \n",
      "\n",
      " \n",
      "Adept in programming languages like \n",
      "Python and\n",
      " \n",
      "SQL\n",
      ".\n",
      " \n",
      " \n",
      " \n",
      "EDUCATIONAL QUALIFICATION\n",
      ":\n",
      " \n",
      " \n",
      "Name of the \n",
      "Course\n",
      " \n",
      "Name of the Institution\n",
      " \n",
      "     \n",
      "Board/                    \n",
      "University\n",
      " \n",
      " \n",
      "Year of                                       \n",
      " \n",
      "  \n",
      "Passing\n",
      " \n",
      "Percentage\n",
      " \n",
      " \n",
      "B.Tech\n",
      " \n",
      " \n",
      "Indira gandhi institute of \n",
      "technology,Dhenkanal,Odisha\n",
      " \n",
      "BIJU PATTNAIK \n",
      "UNIVERSITY OF \n",
      "TECHNOLOGY,\n",
      " \n",
      "ROURKELA\n",
      " \n",
      "2019\n",
      " \n",
      "78%\n",
      " \n",
      "XII\n",
      "th\n",
      " \n",
      "Bhadrak junior college, Bhadrak\n",
      " \n",
      "CHSE(Odisha)\n",
      " \n",
      "2015\n",
      " \n",
      "82.5%\n",
      " \n",
      "                 \n",
      "X\n",
      "th\n",
      " \n",
      " \n",
      "Utakal mani ucha bidya \n",
      "pitha,Bandhatia\n",
      " \n",
      "BSE(Odisha)\n",
      " \n",
      "2013\n",
      " \n",
      "88.7%\n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "KEY SKILLS\n",
      ":\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Python                                                          \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Machine learning                                          \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Statistics\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "EDA\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "NLP\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "MS SQL Server\n",
      " \n",
      "\n",
      "      \n",
      " \n",
      " \n",
      "  \n",
      "TECHNICAL SKILLS:  \n",
      ".\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "Python\n",
      " \n",
      "-\n",
      " \n",
      "Developed various regression and classification models of ML using pandas, \n",
      "numpy, scikit learn etc, and visualization using Seaborn and MatlabPlot libraries.  \n",
      " \n",
      "\n",
      " \n",
      "Statistics and Mathmetics  \n",
      "-\n",
      " \n",
      "Strong knowledge on Descriptive statistics , Inferential \n",
      "statistics, Probability, ANOVA, Chi\n",
      "-\n",
      "Square test, t\n",
      "-\n",
      "test, z\n",
      "-\n",
      "test, p\n",
      "-\n",
      "value, CI, Hypothesis\n",
      "-\n",
      "test,Sampling, Data distributions, Single and multivariate calculus, Linear algebra \n",
      " \n",
      "\n",
      " \n",
      "Machine Learning\n",
      "-\n",
      "(supervi\n",
      "sed learning)\n",
      " \n",
      "Strong knowledge of various ML regression \n",
      "and classification algorithms such as Linear and logistic regression, Gradient descent, \n",
      "Decision tree, Random forest, SVM , Naive Bayes, KNN ,XGBoost, Lasso and Ridge \n",
      "regression ,Hyperparameter tunig \n",
      "and model performance technique such as Confusion \n",
      "matrix, AUC and ROC , IG ,Gini, Confidence Interval, ANOVA,RMSE and also have \n",
      "knowledge on validating the model by Cross Validation.\n",
      "(unsupervised learning)\n",
      "-\n",
      "K\n",
      "-\n",
      "means clustering,  DBSCAN clustering, Heirarchca\n",
      "l clustering\n",
      " \n",
      "\n",
      " \n",
      "NLP(Nltk/spaCy) \n",
      "-\n",
      " \n",
      "Tokenization, Stemming, Lematization ,Stop Words, Part of Speech , \n",
      "Named\n",
      "-\n",
      "entity recognition , Bag\n",
      "-\n",
      "of\n",
      "-\n",
      "Words, TF\n",
      "-\n",
      "IDF, Word2vec, Text Summerization\n",
      " \n",
      " \n",
      "PROJECTS:  \n",
      " \n",
      " \n",
      "PROJECT 1\n",
      ": \n",
      "Customer Feedback Analysis\n",
      " \n",
      " \n",
      "DURATION:\n",
      "May 2021 \n",
      "-\n",
      " \n",
      "Till Date\n",
      " \n",
      " \n",
      "OVERVIEW:\n",
      " \n",
      "Feedback  analysis involves identifying  the needs  and frustrations  of customers, \n",
      "so that businesses can improve customer satisfaction and reduce churn .It's often done \n",
      "automatically, enabling companies to sort huge amounts of data from various channels\n",
      " \n",
      "in a \n",
      "timely and accurate way. An NLP engine scrapes customers' conversations and feedback on \n",
      "websites, social media channels and mobile apps etc. and then analyze these conversations. \n",
      "Depending on the intelligence required, the NLP engine can conduct \n",
      "bina\n",
      "ry(positive/negative)analysis and sentiment analysis on various emotions like \n",
      "Happy,Angry,Sadness and Trust etc. \n",
      " \n",
      " \n",
      "RESPONSIBILITIES:\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "Cleansing and transforming the text dataset to create a comprehensive dataset which \n",
      "contain all the possible hypothesis.\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "Text preprocessing is one of the most important step in the field of Natural language \n",
      "processing .So i clean the text data by removing \n",
      "puncuations , stopwords and then covert \n",
      "the text data into lower case , apply lemmatization \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "After clean the text data then i performed data transformation because machine can't \n",
      "understands the text data so i use Bag of Words ,which converts all the text \n",
      "data into \n",
      "numerical dataset \n",
      " \n",
      " \n",
      "TECHNOLOGIES USED:\n",
      " \n",
      "Python ,Spyder, Sklearn, NLP, NLTK, spaCy.\n",
      " \n",
      " \n",
      "PROJECT 2\n",
      ": \n",
      "Loan Defaulter Risk Analysis\n",
      " \n",
      " \n",
      "DURATION: \n",
      " \n",
      "Feb 2020 \n",
      "-\n",
      "Mar 2021\n",
      " \n",
      " \n",
      "ROLE: \n",
      " \n",
      "ML Engineer\n",
      " \n",
      " \n",
      "OVERVIEW:\n",
      " \n",
      "The goal of this project is to build a machine learning model that can predict if \n",
      "a person will default on the loan based on the loan and personal information. Also this \n",
      "project aims to develop an understanding of risk analytics in banking and financial s\n",
      "ervices \n",
      "and understand how data is used to minimize the risk of losing money while lending to \n",
      "customers.\n",
      " \n",
      " \n",
      "RESPONSIBILITIES: \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "Worked on client during projects and understanding their requirement was the best \n",
      "way for me to build  an optimal solutions.\n",
      " \n",
      "\n",
      " \n",
      "As my\n",
      " \n",
      "dataset is supervised learning and my problem statement is classification \n",
      "problem that's why i used classification\n",
      ".\n",
      " \n",
      "\n",
      " \n",
      "Data preprocessing steps are done \n",
      "-\n",
      " \n",
      "Acquire the dataset, import the required \n",
      "libraries,import the dataset,indentify and handling missing values,encoding the \n",
      "categorical variable,spliting the dataset, feature scaling.\n",
      " \n",
      "\n",
      " \n",
      "Apply machine learning classification model like \n",
      "-\n",
      "Logistic regression, SVC, Decision \n",
      "tree, Random forest, Xgboost.\n",
      " \n",
      " \n",
      "TECHNOLOGIES USED: \n",
      " \n",
      "Python, EDA , Data Visualization,  Data Cleaning, Classification \n",
      "algorithms\n",
      " \n",
      " \n",
      " \n",
      "STRENGTH:\n",
      " \n",
      "\n",
      " \n",
      "Hard working\n",
      " \n",
      "\n",
      " \n",
      "Self motivated\n",
      " \n",
      "\n",
      " \n",
      "Positive attitude\n",
      " \n",
      "\n",
      " \n",
      "Quick learner\n",
      " \n",
      "\n",
      "HOBBIES:\n",
      " \n",
      "\n",
      " \n",
      "Listening songs\n",
      " \n",
      "\n",
      " \n",
      "Playing games\n",
      " \n",
      "PERSONAL PROFILE\n",
      ":\n",
      " \n",
      " \n",
      "       \n",
      "\n",
      " \n",
      "    \n",
      " \n",
      " \n",
      ":  Chakradhar sahoo\n",
      " \n",
      "       \n",
      "Date of Birth             \n",
      " \n",
      " \n",
      ":  1st may 1998\n",
      " \n",
      "       \n",
      "Gender                    \n",
      " \n",
      " \n",
      " \n",
      "            \n",
      ":   Male\n",
      " \n",
      " \n",
      "       \n",
      "Language Known      \n",
      " \n",
      " \n",
      " \n",
      ":  English,Hindi,Odia\n",
      " \n",
      "       \n",
      "Permanent Address                    \n",
      " \n",
      ":  At:\n",
      "-\n",
      "Radhaballav pur, Post:\n",
      "-\n",
      "Bandhatia,\n",
      " \n",
      "                                                                    \n",
      "Via:\n",
      "-\n",
      "Dhamnagar, Bhadrak, Odi\n",
      "sha,\n",
      " \n",
      "                                                                    \n",
      "Pin code:\n",
      "-\n",
      "  \n",
      "756117,INDIA        \n",
      " \n",
      "       \n",
      "Present Address                                :  B\n",
      "-\n",
      "201, Ostia Destination, Moshi, Pune,MH\n",
      " \n",
      "    \n",
      "                                                \n",
      "               \n",
      "Pin code:\n",
      "-\n",
      "  \n",
      "412105, INDIA        \n",
      " \n",
      " \n",
      "  \n",
      "DECLARATION:\n",
      " \n",
      " \n",
      "I hereby declare that the above particulars furnished by me are true to the best of my knowledge \n",
      "and belief. \n",
      " \n",
      "                            \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "                               \n",
      "   \n",
      "Anil Sahoo        \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a pdf file object \n",
    "pdfFileObj = open(r'C:\\Users\\manikantar\\Downloads\\AnilSahoo[2_2].pdf', 'rb') \n",
    "    \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "\n",
    "# print(pdfReader)\n",
    "# printing number of pages in pdf file \n",
    "print(pdfReader.numPages)\n",
    "    \n",
    "# # creating a page object\n",
    "i=0\n",
    "for i in range(0, pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(i)\n",
    "    i=i+1\n",
    "    \n",
    "# # extracting text from page \n",
    "    print(pageObj.extractText())\n",
    "    \n",
    "# # closing the pdf file object \n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ShellError",
     "evalue": "The command `antiword C:\\Users\\manikantar\\Downloads\\SunanditaGhosh[5_0].doc` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             pipe = subprocess.Popen(\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1312\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MANIKA~1\\AppData\\Local\\Temp/ipykernel_1112/1675328584.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtextract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\manikantar\\Downloads\\SunanditaGhosh[5_0].doc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\site-packages\\textract\\parsers\\__init__.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(filename, input_encoding, output_encoding, extension, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiletype_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, filename, input_encoding, output_encoding, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# output encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbyte_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0municode_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\site-packages\\textract\\parsers\\doc_parser.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(self, filename, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'antiword'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Prescreening\\lib\\site-packages\\textract\\parsers\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;31m# File not found.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# This is equivalent to getting exitcode 127 from sh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 raise exceptions.ShellError(\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m127\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 )\n",
      "\u001b[1;31mShellError\u001b[0m: The command `antiword C:\\Users\\manikantar\\Downloads\\SunanditaGhosh[5_0].doc` failed with exit code 127\n------------- stdout -------------\n------------- stderr -------------\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "text = textract.process(r'C:\\Users\\manikantar\\Downloads\\SunanditaGhosh[5_0].doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Buchhibabu Rachakonda\\n\\n\\t\\n\\n\\t\\n\\n\\t\\t\\n\\n\\t\\tAddress: Hyderabad, India\\n\\n\\t\\tContact:  +91-9177304780\\n\\nEmail:  buchhibabu97@gmail.com\\n\\nLinkedIn: https://www.linkedin.com/in/buchhibabu-rachakonda-748500a2 \\n\\n\\t\\n\\n\\t\\n\\n\\tSummary:\\n\\n3+ years of experience of Python and C#  Developer\\n\\nExperience in automating tasks using Python and C# (AI and non AI methods) \\n\\nExpertise in OOPS to design scalable and robust code which can withstand multiple changes in requirements\\n\\nUsed Django and flask , MongoDB, REST API, SQL\\n\\nExpertise in doing data wrangling using NumPy, Pandas and matplotlib\\n\\nExposure in Keras and tensor flow for image segmentation and image labelling tasks.\\n\\nExposure in data visualization with various visualization libraries like Matplotlib, Seaborn\\n\\nExposure in image processing libraries like OpenCV, CNN\\xe2\\x80\\x99s, Yolov3. \\n\\nGreat Exposure of image processing algorithms, Lidar processing using both AI and Non \\xe2\\x80\\x93 AI algorithms \\n\\nMicrosoft certified Azure fundamentals\\n\\nCertified as Azure Champion by the current company\\n\\nDeep understanding of multiple supervised, unsupervised algorithms, scikit-learn library\\n\\nIn depth exposure to multiple reinforcement learning algorithms and its use cases\\n\\nExperience with Agile Scrum Methodology.\\n\\nIn depth understanding of Sensor Fusion\\n\\nTaught C and Data structures for college going undergrads in non-honorarium role for a company\\n\\nExposure to JavaScript, HTML5 and CSS\\n\\nExposure to Flask, MongoDB, Django and REST API\\n\\nExposure to Time series data\\n\\n\\tTechnical Skills & Tools:\\n\\n\\tSkills: Python, C++, C, NumPy, Keras, TensorFlow, Image processing algos, OpenCV, CNN\\xe2\\x80\\x99s, Lidar processing, PCL library, Azure certified, scikit-learn, beautiful soup, html5, JavaScript, CSS, Data structures and algorithms, Core- Java.\\n\\n\\tExperience:\\n\\nTata Consultancy Services (Digital) (Dec 2018 \\xe2\\x80\\x93 present)\\n\\n\\tRole: Python Developer (AI-ML)\\n\\nClient: Leading Tier \\xe2\\x80\\x93 1 OEM company\\n\\nLocation: Hyderabad, India\\n\\nDuration: Dec 2018 to present\\n\\n\\n\\nProject \\xe2\\x80\\x93 1: Developer : ( Jan 2019 to Oct 2019 )\\n\\nResponsibilities:  \\n\\nUsed Natural language processing libraries like NLTK to automate tasks\\n\\nUsed RNN and LSTM for synchronizing the validation tasks.\\n\\nExposure to Transformers, SeqtoSeq models, Genism, HuggingFace, Glove models\\n\\nUnderstand the requirements, Create the workflow, develop the code with standard OOPs concepts and test it accordingly and deploy into multiple systems in C# and python.\\n\\nUnderstood the workflow of test procedure validation and automated the pipeline up to 75 % \\n\\nUsed Pandas and Numpy libraries to do data processing.\\n\\nDeveloped scripts to automate mundane tasks in excel word with standard OOPs concepts either in C++ and Python\\n\\nCreated own tools/Scripts to continuously develop, test and integrate code \\n\\n\\n\\nProject 2 : Developer : (Oct 2019 to Jan 2020)\\n\\nResponsibilities: \\n\\nMaintained a Multithreading/Multiprocessing application, continuously solved CR\\xe2\\x80\\x99s and improved the tool.\\n\\nSolved the critical multithreading issues by refactoring certain parts and inducing modularity for certain functions of the application.\\n\\n\\n\\nProject 3: Sensor Fusion Engineer: (Jan 2020 to present):\\n\\nWorked in a cross-functional team, where every step has dependency on others\\n\\nUsed Django, Flask, MongoDB and SQL with Cloud\\n\\nWorking as sensor fusion engineer where we associate and unify all the sensor outputs and fed them to reinforcement learning agents\\n\\nWorked with Open-CV library for morphological operations on road\\n\\nWorked in Computer vision related tasks like image segmentation of objects in an image\\n\\nWorked on Object Detection, Object localization using CNN and Yolo-V3 models \\n\\nExposure to RCNN, SDD Models \\n\\nCreated and trained models from scratch for custom signs detection\\n\\nUsed Data Augmentation Techniques while re-training\\n\\nWorked in lane detection team , used Sci-kit learn, Tensorflow, keras , DBSCAN , Unsupervised Algorithms.\\n\\nWorked in lidar object detection team, used Numpy, pandas, sea-born, matplotlib to understand, process and produce meaningful data for training.\\n\\nUsed Azure scalable VM\\xe2\\x80\\x99s , storage blobs, created pipelines , unified them to automate the labelling procedures.\\n\\n\\n\\n\\tEducational Qualification:\\n\\n\\tStandard\\n\\n\\tDepartment\\n\\n\\tInstitution\\n\\n\\tYears\\n\\n\\tPercentage\\n\\n\\tB.E\\n\\n\\tElectronics and Communication\\n\\n\\tMatrusri Engineering college (Osmani University)\\n\\n\\t2014-2018\\n\\n\\t62.8 %\\n\\n\\tIntermediate\\n\\n\\t(BIEAP)\\n\\n\\tMPC\\n\\n\\tNarayana Junior college, HYD\\n\\n\\t2012-2014\\n\\n\\t84.5%\\n\\n\\t10TH (BSEAP)\\n\\n\\tGeneral\\n\\n\\tNarayana Concept School, Hyd.\\n\\n\\t2012\\n\\n\\t85.4%\\n\\n\\t\\n\\n\\tMajor project (B.E):\\n\\n\\t Transparent audio watermarking using Fibonacci series:\\n\\n\\tIt\\xe2\\x80\\x99s a military grade encryption technique which encrypts a text in an image and then in turn hide the image in specific places of audio using the magic properties of Fibonacci series.\\n\\n\\tPublished the paper in IJCRT:\\n\\n\\thttps://ijcrt.org/papers/IJCRTNTSE004.pdf\\n\\n\\tHyderabad.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docx to txt convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_directory = os.path.join(os.getcwd(), 'G:/PreScreening/ewfs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for process_file in  os.listdir(source_directory):\n",
    "    file, extension = os.path.splitext(process_file)\n",
    "    \n",
    "    # We create a new text file name by concatenating the .txt extension to file UUID\n",
    "    dest_file_path = file + '.txt'\n",
    "    \n",
    "    #extract text from the file\n",
    "    content = textract.process(os.path.join(source_directory, process_file))\n",
    "    \n",
    "    # We create and open the new and we prepare to write the Binary Data which is represented by the wb - Write Binary\n",
    "    write_text_file = open(os.path.join(training_directory, \"G:/PreScreening/ewfs/txt\"), \"wb\")\n",
    "    \n",
    "    #write the content and close the newly created file\n",
    "    write_text_file.write(content)\n",
    "    write_text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pdf file object \n",
    "pdfFileObj = open(r'C:\\Users\\manikantar\\Downloads\\AnilSahoo[2_2].pdf', 'rb') \n",
    "    \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "\n",
    "# print(pdfReader)\n",
    "# printing number of pages in pdf file \n",
    "print(pdfReader.numPages)\n",
    "    \n",
    "# # creating a page object\n",
    "i=0\n",
    "for i in range(0, pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(i)\n",
    "    i=i+1\n",
    "    \n",
    "# # extracting text from page \n",
    "    print(pageObj.extractText())\n",
    "    \n",
    "# # closing the pdf file object \n",
    "pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertion all Formats to IMAGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import aspose.words as aw\n",
    "import watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='G:/PreScreening/resumes/New folder (2)/'\n",
    "save_directory= 'G:/PreScreening/resumes/New folder/'\n",
    "for filename in os.listdir(directory):\n",
    "    save_name = ''.join(filename.split(\".\")[:-1])\n",
    "    if filename.endswith('.pdf'):\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        images = convert_from_path(fullpath, 500)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(save_directory + save_name+\"_\"+str(i)+\".png\")\n",
    "    if filename.endswith('.doc'):\n",
    "       paths = glob(r'C:\\\\Users\\\\manikantar\\\\Downloads\\\\BA_BRD*.doc', recursive=True) \n",
    "    if filename.endswith('.docx'):\n",
    "        save_name = ''.join(filename.split(\".\")[:-1])\n",
    "#         doc=aw.Document(os.path.join(directory,filename))\n",
    "#         for page in range(0, doc.page_count):\n",
    "#             extractedPage = doc.extract_pages(page, 1)\n",
    "#             fn = filename +str(page)+'.PNG'\n",
    "#             extractedPage.save(save_directory + fn, \"PNG\")\n",
    "        doc = aw.Document(os.path.join(directory,filename))       \n",
    "        for page in range(0, doc.page_count):\n",
    "            extractedPage = doc.extract_pages(page, 1)\n",
    "            extractedPage.save(save_directory + save_name+\"_\"+str(page)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kota priya nandini.docx\n",
      "Naukri_Vijay[6y_0m].docx\n",
      "SunanditaGhosh[5_0].docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "direcc = r'C:/Users/manikantar/Downloads/New folder (3)/'\n",
    "\n",
    "clean =  r'C:/Users/manikantar/Downloads/New folder (3)/'\n",
    "img = cv2.imread(filename)\n",
    "for filename in os.listdir(direcc):\n",
    "    print(filename)\n",
    "    if filename.endswith('.png'):\n",
    "        save_name = ''.join(filename.split(\".\")[:-1])\n",
    "        print(save_name)\n",
    "        alpha = 2.0\n",
    "        beta = -160\n",
    "            #new = alpha * img + beta\n",
    "        new = np.clip(new, 0, 255).astype(np.uint8)\n",
    "        cv2.imwrite(clean + save_name + \".jpg\", new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=\"itojafldkjqopeialgmfaoruovaoigidjv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MANIKA~1\\AppData\\Local\\Temp/ipykernel_18376/499717447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'char' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ITOJAFLDKJQOPEIALGMFAORUOVAOIGIDJV'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
